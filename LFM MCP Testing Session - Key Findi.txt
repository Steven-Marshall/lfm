LFM MCP Testing Session - Key Findings & Improvements

  Date: 2025-10-12Context: Testing lfm MCP service with Claude Code in thinking/planning mode for music
  recommendations

  Test Case: Taylor Swift Listening Analysis

  Task

  User requested 2 album recommendations based on Taylor Swift listening, noting "I'm not really into mainstream
  pop"

  Approach Used

  1. Analyzed TS album/track listening (deep searches)
  2. Examined top 50 artists overall
  3. Checked recent listening patterns
  4. Manually reasoned about non-mainstream appeal
  5. Verified candidate albums hadn't been heard

  Recommendations Delivered

  1. Joni Mitchell - Hejira (1976) - Historical influence on folklore/evermore aesthetic
  2. Joanna Newsom - Ys (2006) - Literary storytelling + baroque arrangements matching classical taste

  Key Insight

  User's TS listening showed:
  - Top albums: folklore (455), 1989 (491), reputation (394)
  - Favorite tracks: indie-leaning storytelling ("exile", "the last great american dynasty", "my tears ricochet")
  - Broader taste: experimental rock (These New Puritans, Black Country New Road), classical (Maria Callas),
  sophisticated pop (Beatles, Beach Boys)

  Conclusion: Attracted to TS's non-mainstream elements (Aaron Dessner production, literary lyrics, emotional
  complexity), not pop polish

  ---
  Critical Bug: Config File Locking

  Issue

  Multiple parallel MCP calls cause file locking conflicts on config.json

  Error Details

  fail: Lfm.Core.Configuration.ConfigurationManager[0]
        Error saving configuration to C:\Users\steve\AppData\Roaming\lfm\config.json
        System.IO.IOException: The process cannot access the file because it is being used by another process.
           at ConfigurationManager.SaveAsync(LfmConfig config) line 151

  warn: Lfm.Core.Services.CachedLastFmApiClient[0]
        Cache cleanup failed, continuing normally
        System.IO.IOException: The process cannot access the file because it is being used by another process.
           at CachedLastFmApiClient.TryCleanupIfNeededAsync() line 566

  Trigger

  Made 4 parallel tool calls simultaneously:
  - lfm_artist_albums (deep)
  - lfm_artist_tracks (deep)
  - lfm_artists
  - lfm_recent_tracks

  Impact

  - Operations complete successfully ("continuing normally")
  - Creates significant noise in output
  - User experience degradation

  Potential Fixes

  1. Add file locking/retry logic with exponential backoff
  2. Queue config writes instead of immediate writes
  3. Only write config on actual changes, not every request
  4. Use FileShare.ReadWrite when opening config file
  5. Consider in-memory config with periodic persistence

  ---
  Improvement Discussion

  1. Performance (Deep Searches)

  Issue: Deep artist/album searches are slow (45K+ track searches, 180K total scrobbles)

  Current State:
  - deep: true searches entire history exhaustively
  - depth: N parameter exists but not well documented
  - Progress messages stream but aren't actionable for LLM

  Recommendations:
  - Enhance guidelines with clear examples:
  - depth: 500 for quick recent history (1-2 sec)
  - depth: 2000 for broader check (5-10 sec)
  - deep: true for exhaustive search (30+ sec for large libraries)
  - Smart defaults: only go deep: true if item not found in first N results
  - Upfront warning in tool response when slow operation expected
  - Note: Estimated time doesn't help LLM, but setting expectations does

  2. Error Noise

  Status: See bug section above - needs file locking fix

  3. Discovery Gap

  Issue: Had to manually brainstorm candidate artists instead of using tools

  Available Tools:
  - lfm_similar - Find similar artists to given artist
  - lfm_recommendations - Get recommendations based on listening history with filters

  Test Results:
  Called lfm_similar artist="Taylor Swift" and got:
  - Sabrina Carpenter, Olivia Rodrigo, Gracie Abrams, Harry Styles, Ariana Grande
  - Basically mainstream pop (genre + popularity matching)
  - Phoebe Bridgers appeared at #22 (0.25 match)

  Called lfm_recommendations period="overall" filter=1:
  - Classical/opera (driven by Maria Callas listening)
  - Ringo Starr (Beatles/Beach Boys connection)
  - OneRepublic (Coldplay connection)

  Conclusion:
  Algorithmic recommendations surface correlation ("people who like X also like Y") but miss causation ("you like
  folklore's storytelling, here's the historical source material").

  Design Philosophy Validated:
  Tools provide data infrastructure â†’ LLM applies reasoning about musical lineage, aesthetic connections, and
  non-obvious patterns. Combination is more powerful than either alone.

  Potential Enhancement:
  Could still use similar/recommendations as starting point to validate/augment manual reasoning.

  4. Tool Naming Clarity

  Issue: Similar tool names with different purposes caused confusion

  Current:
  - lfm_tracks - top tracks info (all artists)
  - lfm_toptracks - create playlist from top tracks
  - lfm_artist_tracks - deep search for specific artist

  Suggestions:
  - lfm_top_tracks_info
  - lfm_create_playlist_from_top
  - lfm_search_artist_tracks

  Or enhance descriptions to clarify use cases more explicitly.

  5. Album Comparison Tool

  Challenge: Track count variations make direct album comparisons difficult

  Examples Analyzed:

  Pink Floyd - Wish You Were Here

  - 57 plays total, 5 tracks
  - Track sum: 41 plays
  - Unaccounted: 16 plays
  - "Shine on You Crazy Diamond" (both parts): 0 plays shown
  - "Wish You Were Here" (title): 24 plays

  Reality: ~8-9 complete album listens + 16 extra plays of title track separately

  Sufjan Stevens - Illinois

  - 617 plays total, 22 tracks (74 minutes)
  - Track sum: 393 plays
  - Unaccounted: 224 plays (36%!)
  - 9 tracks showing 0 plays
  - Range: 9-53 plays per track

  Issues:
  - Long track titles get truncated by Last.fm (metadata fragmentation)
  - Album length causes "drop-off effect" (life interruptions)
  - Positional effects (front-loaded listening)

  Reality: ~20-28 complete listens, many partial listens, some replayed favorites

  Taylor Swift - folklore

  - 455 plays total, 17 tracks
  - Track sum: 415 plays
  - Unaccounted: 40 plays
  - Track 1: 33 plays â†’ Track 17: 18 plays (45-50% drop-off)
  - "exile": 0 plays (but 40 unaccounted = "feat. Bon Iver" variant)
  - "the lakes": 0 plays (bonus track not owned)

  Key Patterns Identified:

  Zero-play tracks mean:
  1. With high unaccounted plays â†’ metadata variant (featuring artists, title differences)
  2. At end of long album â†’ drop-off effect from interruptions
  3. On bonus/deluxe editions â†’ user doesn't own that version

  Current Approach is Correct:
  - Provide raw data (plays, track breakdown, unaccounted plays, discrepancy flags)
  - Trust LLM to synthesize contextually
  - Don't over-engineer algorithms that can't account for real-world listening behavior

  Data is messy at source (Last.fm metadata chaos) - acknowledge rather than pretend to clean perfectly.

  Potential Additions to Guidelines:
  ### Understanding Zero-Play Tracks:
  - With high unaccounted plays â†’ metadata variant
  - At end of long album â†’ drop-off effect
  - On bonus/deluxe track â†’ may not own that version

  ### Album Length Effects:
  - Long albums (60+ min, 15+ tracks) show drop-off patterns
  - Compare track 1 vs final track to gauge completion rate
  - Variance indicates both favorites AND listening context

  6. Guidelines Length & Delivery

  Current State:
  - ~2000 lines returned by lfm_init
  - Comprehensive and well-written
  - Token budget isn't the issue (200K available)

  Real Issue: Attention & Relevance

  What worked:
  - Temporal parameters guidance
  - "Be a DJ buddy" framing
  - Metadata mismatch explanations

  What got skimmed:
  - Playback workflows (not relevant to task)
  - Troubleshooting details
  - Some parameter details

  The "Momentum Problem"

  Without planning mode, LLM behavior:
  Get data â†’ See interpretation â†’ Use it â†’ Next step
  [IGNORE EMBEDDED GUIDANCE]

  Historical Context:
  - Developer originally embedded contextual guidance in tool results
  - e.g., lfm_artist_albums returned: "Read guidelines to understand playcount data correctly"
  - LLMs completely ignored these instructions without planning mode
  - Forced workaround: front-load everything via lfm_init

  Planning Mode Changes This:
  Get data â†’ [PAUSE - what does this mean?] â†’ Reference guidelines â†’ Think through patterns â†’ Act

  Recommended Experiment:

  Try returning to original architecture with planning mode:

  1. Make lfm_init return condensed version (~500 lines):
    - Key principles
    - Tool selection overview
    - Response style guidance
  2. Embed contextual guidance in tool results:
  {
    "album": "Wish You Were Here",
    "userPlaycount": 57,
    "unaccountedPlays": 16,
    "guidanceNote": "ðŸ’¡ 16 unaccounted plays suggest metadata variants. With 5 tracks, this indicates ~8-11 complete
   listens. Track variance may indicate favorites or drop-off effects on longer albums."
  }
  3. Add system prompt for planning mode:
    - "When you see guidance flags, use thinking blocks to process them"
    - "Before responding, think through what data patterns reveal"
    - "Pause at checkpoints: task approach, data interpretation, recommendation logic"
  4. Optional: Add verbose guidelines tool:
    - lfm_guidelines topic="metadata" for deep dives
    - lfm_guidelines verbose=true for full reference

  Design Philosophy:
  Raw data + contextual guidance + forced reflection > front-loaded encyclopedia

  ---
  Key Takeaways

  What Works Well

  1. Data infrastructure is solid - Deep searches, bulk checks, verbose breakdowns provide everything needed
  2. Design philosophy is correct - Tools provide data, LLM provides reasoning
  3. Guidelines content is valuable - Well-written and comprehensive
  4. Reasoning over algorithms - Manual analysis produced better recommendations than lfm_similar

  Critical Issues

  1. Config file locking bug - Needs immediate fix for parallel calls
  2. Delivery mechanism - Front-loading guidelines was a workaround, not optimal design

  Promising Directions

  1. Planning mode as solution - May allow return to cleaner just-in-time guidance architecture
  2. Contextual guidance in results - Worth retrying with planning mode enabled
  3. Trust the messiness - Don't over-engineer normalization algorithms, let LLM reason through patterns

  Design Principles Validated

  - Clean primitives over complex algorithms - Provide raw data with metadata, trust LLM synthesis
  - Acknowledge data messiness - Last.fm metadata is chaotic, don't pretend otherwise
  - Context-appropriate reasoning - LLM can understand drop-off effects, favorites, metadata variants in context
  - Reasoning beats correlation - Understanding why someone likes something > algorithmic similarity

  ---
  Next Steps

  1. Fix config locking bug (high priority)
  2. Experiment with planning mode + contextual guidelines
  3. Consider condensing lfm_init, enriching tool responses
  4. Potentially add examples to guidelines about zero-play interpretation
  5. Clarify tool naming or enhance descriptions
